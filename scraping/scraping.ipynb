{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "# Import requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    # Fetch the page that we're going to parse\n",
    "    page = requests.get(url)\n",
    "    # Parse the page with beautiful soup\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_title(soup):\n",
    "    title = soup.find('h1').text\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_author(soup):\n",
    "    author = soup.find('h2', class_='pw-author-name').text\n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_date(soup):\n",
    "    date = soup.find('p', class_='pw-published-date').text\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reading time of the article\n",
    "def get_article_reading_time(soup):\n",
    "    content = soup.find('div', class_='pw-reading-time').text\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaks the medium article into sections and sub sections based on headers\n",
    "def get_article_content(soup):\n",
    "    # Get the content of the article\n",
    "    content = soup.findAll(class_='pw-post-body-paragraph')\n",
    "    content = [x for x in content]\n",
    "    heading_tag = 'h1'\n",
    "    headings = soup.findAll(heading_tag)\n",
    "    # Fix for articles with only one main heading\n",
    "    if(len(headings) < 2):\n",
    "        heading_tag = 'h2'\n",
    "        headings = soup.findAll(heading_tag)\n",
    "    sections = {}\n",
    "    for heading in headings:\n",
    "        section = {}\n",
    "        key = '_content_'\n",
    "        inner_section = []\n",
    "        temp = heading\n",
    "        while(True):\n",
    "            check = temp.next_sibling\n",
    "            if check is None:\n",
    "                break\n",
    "            if check.name == heading_tag:\n",
    "                break\n",
    "            if check.name == 'h2':\n",
    "                if len(inner_section) > 0:\n",
    "                    section[key] = ' '.join(inner_section)\n",
    "                key = check.text\n",
    "                inner_section = []\n",
    "            temp = check\n",
    "            if check.name == 'p':\n",
    "                inner_section.append(check.text)\n",
    "        if(len(inner_section) > 0):\n",
    "            section[key] = ' '.join(inner_section)\n",
    "        if len(section) > 0:\n",
    "            sections[heading.text] = section\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def fetch_summary(sections):\n",
    "    final_summary = \"\"\n",
    "    article_length = 0\n",
    "    for section_count, section in enumerate(sections):\n",
    "        display(Markdown(f\"## {section}\"))\n",
    "        temp_prompt = \"\"\n",
    "        for subsection_count, key in enumerate(sections[section]):\n",
    "            prompt = sections[section][key]\n",
    "            if(key != '_content_' and len(prompt.split(\" \")) > 100):\n",
    "                display(Markdown(f\"### {key}\"))\n",
    "            temp_prompt += prompt\n",
    "            if(len(temp_prompt.split(\" \")) < 100 and section_count != len(sections) - 1 and subsection_count != len(sections[section]) - 1):\n",
    "                continue\n",
    "            article_length += len(temp_prompt.split(\" \"))\n",
    "            response = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                prompt=f\"Summarize the following text. Do not leave any sentences incomplete. If you reach the maximum length, remove the incomplete sentence: \\n\\n{temp_prompt}\",\n",
    "                temperature=0.7,\n",
    "                top_p=1.0,\n",
    "                max_tokens=150,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=1\n",
    "            )\n",
    "            summary = response['choices'][0]['text']\n",
    "            if len(summary) > 0:\n",
    "                display(Markdown(f\"{summary}\\n\"))\n",
    "                final_summary += summary\n",
    "            temp_prompt = \"\"\n",
    "    return final_summary, article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article URL\n",
    "def display_summary(url):\n",
    "    soup = get_soup(url)\n",
    "    title = get_article_title(soup)\n",
    "    author = get_article_author(soup)\n",
    "    date = get_article_date(soup)\n",
    "    sections = get_article_content(soup)\n",
    "    display(Markdown(f\"# {title}\"))\n",
    "    display(Markdown(f\"Written by **{author}** on **{date}**\\n\\n\"))\n",
    "    final_summary, article_length = fetch_summary(sections)\n",
    "    print(f\"Length of article: {article_length} words\")\n",
    "    print(f\"Length of final summary: {len(final_summary.split(' '))} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# The Artistic Potential of AI: Understanding DALL-E 2 and Stable Diffusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Written by **Machine Minds** on **Feb 11**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## An Overview"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "AI is transforming the digital world by allowing machines to act and think like humans. Generative AI takes this a step further, introducing the ability to create original visuals, text, sounds, or songs. Models such as DALL-E 2 and Stable Diffusion, developed by OpenAI and LMU Munich respectively, use transformer-based designs and extensive training data to generate images from verbal descriptions, pushing the boundaries of creativity. AI has something to offer for everyone, with endless possibilities in store.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## The Technology Behind DALL-E 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Neural Network Architecture"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "DALL-E 2 is a transformer-based deep learning model developed by OpenAI to generate good-quality images from verbal descriptions. It is highly advanced, trained on a large dataset of images, and can create realistic or stylized images based on input text. The architecture consists of an input embedding layer (12 layers of multi-head self-attention mechanism and feed-forward module), transformer layers, feed-forward network, and output layer (convolutional/deconvolutional layers). The two main technologies powering it are CLIP (Contrastive Language-Image Pre-training) and multi-head self-attention mechanism.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CLIP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "CLIP is an embedding model that creates a latent space representing text and images. It allows us to create variations of images by moving in the latent space, where certain features will change significantly while others remain the same. It is trained with a contrastive objective function, rewarding correct pairs and penalizing incorrect ones. The final result is a model that can be used to map a text description to a representation of an image, and perform zero-shot prediction - i.e. predicting unseen categories of data.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Diffusion Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "DALL-E 2 uses a diffusion model comprising of a decoder and a prior to convert a CLIP text embedding to an image. The decoder non-deterministically converts an image embedding to an actual image while the prior converts a CLIP text embedding to a CLIP image embedding. Two upscaling models are then used to generate a high resolution image from the result.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Comparison between DALL-E 2 and Stable Diffusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The choice between DALL-E 2 and Stable Diffusion depends on the desired outcome; DALL-E 2 provides high-quality images, while Stable Diffusion offers stability and robustness for real-world applications.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Try it yourself"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "OpenAI provides 50 free credits when signing up and 25 extra credits each month for users to create their own images using DALL-E 2. Stable Diffusion is available online for free, as well as a pre-computed model for download, and a Colab Notebook for a code-based implementation. Midjourney is another image generation model with 25 credits given to new Discord accounts, and the Discord Bot can be invited to any server.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Generative AI is a rapidly expanding field within Artificial Intelligence that has enabled the creation of unique images based on natural language prompts. OpenAI's DALL-E 2 is an advanced model utilizing CLIP and diffusion models, showcasing the potential to revolutionize the digital world. Today, anyone can use this cutting-edge technology to bring their ideas to life.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of article: 1799 words\n",
      "Length of final summary: 455 words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://medium.com/sfu-cspmp/the-artistic-potential-of-ai-understanding-dall-e-2-and-stable-diffusion-10a82e6965c0\"\n",
    "display_summary(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Stop using notebooks (or why you should start making data science like a developer)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Written by **Johan Jublanc** on **Jun 11, 2022**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## What are “notebooks” used for and why they are not effective ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Why do we like notebooks as data scientists?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Notebooks are often used at the beginning of data science projects to explore solutions, make a proof of concept and validate technical feasibility. However, they prevent the implementation of good software development practices and require double work when transitioning to production. Additionally, notebooks have four major disadvantages: they are not conducive to the development of complex systems and systematic analysis.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Not a rigorous scientific framework"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Notebooks lack a rigorous framework for scientific approaches and do not facilitate their implementation. There is no easy way to track the evolution of experiments, which makes versioning difficult. Teams often resort to manual versioning systems which are not optimal. Additionally, results appearing may have nothing to do with the actual code in the notebook, making experiments hard to reproduce.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Not a software product development framework"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Lack of development tools in notebooks can make it difficult to quickly and effectively implement good development practices. These practices, such as focusing on one part of the problem, testing the value of the product early, and making data science a developer's task, help save time and design quality. To do this, it is best to develop the solution directly as software.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## A new approach: think of exploration as part of the software development life cycle"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Make data science like a developer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Data scientists no longer need to solve simple machine learning problems and instead must be able to simulate and solve complex ones. In order to do this, they must use scripts and an IDE to explore, manipulate multiple modules and animate them in learning cycles. Versioning code and experiences should be used, with tools like Git and MlFlow Tracking, so the code and results of each experiment can be mapped. Additionally, a new commit should be made when running the code.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Keep the most interesting advantages of notebooks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Data scientists mainly use the language Python to explore and plot graphics. Tools such as Pycharm, notebooks, plotly, pandas-profiling, tensorboard, and MLflow are used for data exploration, and plots can be saved in html pages or other formats. For larger tasks, one can launch computational tasks on a remote machine, use an IDE on a VM, or use strategies to avoid using a huge amount of computing power.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### An effort worth making"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Data science for developers has a cost of entry, but is worth the effort to set up. In three days, data scientists can be familiar with best practices and apply them in their projects. After some time, they are able to start data science projects in their IDE and use good development practices.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Stopping using notebooks is difficult as it requires a habit change and learning new tools, but can be done quickly and save time in the short-term while reducing risks. Additionally, this allows data scientists to collaborate more closely with developers, engineers, and other team members, leading to better alignment with standards and best practices, better understanding within the team, and faster results. The ultimate goal should be to integrate experimentation into an MLOps workflow, and getting rid of notebooks is the first step towards this.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of article: 1470 words\n",
      "Length of final summary: 463 words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://medium.com/@johan-jublanc/stop-using-notebooks-or-why-you-should-start-making-data-science-as-a-developer-would-do-5d67c8990df5\"\n",
    "display_summary(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Measuring the Gender Gap in Animated Films Using Computer Vision"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Written by **Mariana Avelino** on **Feb 17**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## The Rise in Female Representation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This analysis of Disney and Pixar films from 1937-2021 reveals that while the ratio of male to female faces has improved in recent years, there is still an overall underrepresentation of female characters. Grouping the films by decade shows a slow but positive trend towards equal gender representation, with a higher standard deviation in recent decades suggesting greater diversity and less formulaic structures and characters.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## The Main-Character Effect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "By segmenting films based on main character gender, more nuanced insights can be seen. Films with female protagonists show an increasing proportion of female faces while films with male protagonists do not. This means that equality in gender representation is not uniform across films, but rather films with female main characters are creating the positive overall trend. Recently, these films have even become biased towards female faces, which explains why there are so few gender-balanced films, despite a general increase in FFR. Of 51 analyzed films, only 7 met the criteria of 45%-55% female faces.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Female Presence in Groups"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "Analysis of male and female faces in frames of animated films has revealed that movies with male protagonists tend to emphasize male-dominant groupings, such as all-men groups and 1-male and 2-male combinations. As a result, over 55% of frames in these films contain no women. In contrast, frames with three or more females are rare and have only become more common in the last two decades.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Female Prominence"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "An analysis of Disney and Pixar films revealed that large faces (bounding boxes > 96² pixels) accounted for 66% of all faces, with a strong correlation between the percentage of large female faces and the percentage of all female faces. However, the movie Brave (2012) was an outlier, with female faces 10.8% more prominent than male faces. This can be attributed to its plot, which focused on a female protagonist seeking emancipation in a male-dominated world. Films with female protagonists have a higher large-face FFR than the baseline, suggesting equal prominence is given to both genders despite the underlying bias of animation favoring male faces.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Equal vs Fair Representation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The Bechdel Test has been adopted as an official measure to examine fair female representation in animation films. To pass the test, a film must have two female characters who talk to each other about something besides a man. 25% of films passed the test in the first half of the 20th century, while 78% have passed since the turn of the century. This suggests a correlation between fairness and equality of female representation, with animation being faster in converting to fair representation than equal representation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Gender representation in animated films is still unequal, but there are signs of improvement. Female characters often exceed male characters in some films, and it is important not to be too prescriptive about the amount of screen time for either. Films can convey subtle messages that influence how audiences perceive themselves and others, so it is important for genders to be well represented. This could help children see characters they identify with in a variety of roles, inspiring them to reach their full potential.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of article: 1364 words\n",
      "Length of final summary: 497 words\n"
     ]
    }
   ],
   "source": [
    "# Examples of articles\n",
    "\n",
    "url = \"https://medium.com/better-programming/measuring-the-gender-gap-in-animation-using-ai-fac738be4b19\"\n",
    "display_summary(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# MLflow — a modern MLOps tool for data project collaboration"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Written by **DeepThinkers** on **Feb 11**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Introduction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The machine learning lifecycle is a cyclic iterative process with instructions and best practices for developing machine learning algorithms to solve business problems. MLflow is an open-source platform for the complete machine-learning lifecycle that allows users to manage their end-to-end machine-learning workflow. It provides tools to assist with various aspects of the machine learning workflow, making it easier to collaborate and increase velocity in creating projects from experimentation to production.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MLflow Concepts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Mlflow is an MLOps platform which helps to manage and monitor the different phases of a ML model's lifecycle. It provides tracking, project and model features to log, store and visualize model runs and results. It also offers a structured approach to organize ML code and files and a simple API to load models. Moreover, it provides shared storage of all models, versions, data, and experimental artifacts for comparison, deployment and collaboration. This reduces time and effort in streamlining the model development process and bringing it to production.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Traditional methodology vs MLflow"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "MLflow is a tool for managing the entire machine learning workflow, from experimentation to deployment. It provides an end-to-end platform for managing and tracking machine learning experiments, packaging code into reproducible runs, and sharing and deploying models. It replaces ad-hoc methods with an organized system of tools and processes, providing advantages such as better reproducibility and transparency, improved collaboration, enhanced visibility, and faster results.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Quick Start on Local"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Working on the model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "MLflow is a platform that easily allows tracking of components such as model versions, datasets, hyperparameters/parameters, evaluation metrics, and output (trained) model files. It can be installed on Windows or Linux based platforms, with additional ML libraries and tools being optional. After installation, users can test it by running a server locally with port 5000 (http://localhost:5000). An example of using MLflow to track parameters, metrics, and the model to a dashboard is shown when training a decision tree model on the iris dataset. The code for this implementation is simple, with all information about the model runs logged to a directory called mlruns.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### What exactly IS on the MLflow dashboard?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "After deploying a model to MLflow, insights can be drawn by comparing the best results of multiple runs. For example, a parallel coordinate plot can show all the runs reaching the highest accuracy when the parameter leaf_nodes=4. By filtering only these runs, an anomaly was noticed where one counter case (depth = 2) did not reach the highest score, which informed us of the suitable max_depth range for better results.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MLflow deployment in AWS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Prepare MLflow Container Image"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This text describes how to set up MLflow in a production-like environment on AWS ECS. It involves preparing a container image, adding necessary customization, and pushing it to ECR or another public container registry. Additionally, instructions are provided for creating a repository and authenticating the Docker CLI with AWS ECR.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Create an MySQL instance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This text describes how to set up an AWS RDS instance to serve as a back-end store for MLflow. To do this, the MySQL engine option is chosen with the Free Tier template and Public Access turned on. Additionally, a public IP address and VPC Security Group allowing inbound traffic must be configured. Finally, the instance must be connected to and a database and user created for MLflow.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "An S3 bucket is a suitable Blob storage for arbitrary artifacts generated by ML pipelines and logged to MLflow. MLflow stores model files and any extra artifacts in the configured S3 bucket. In AWS ECS, a task definition specifies how ECS should run a container, including the container image, environment variables, port mappings, required resources and more. A task definition must be created in order for MLflow to be used.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "An ECS Cluster is a logical grouping of ECS Services and Tasks, not actual VMs running containers. To create an ECS Service that starts container(s) to run an MLflow tracking server, preparation steps must be completed such as creating stores (RDS and S3), and setting up an ECS Task Definition and Cluster on AWS Fargate. Finally, the mlflow task definition can be deployed by selecting 'Create Service' in the Deploy dropdown button.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Configure Load Balancer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "A Load Balancer (LB) was created in the previous step for the MLflow service. To allow public access, a new Security Group called mlflow-lb-sg must be created with specific firewall rules. After creation, it can be assigned to the LB and accessed publicly from the Internet. Finally, hitting the LB address will reveal a remote MLflow server that can be shared with the team.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Integrate MLflow with AWS SageMaker Pipeline"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Pre-requisite: Creating SageMaker Project"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "AWS SageMaker is a fully-managed Machine Learning service that provides features for data scientists to develop, train, test and deploy ML models. SageMaker Pipeline and MLflow integrate nicely to form a complete solution for building, tracking, monitoring, and deploying ML models. A SageMaker Pipeline can be created from a Jupyter Notebook using AWS SageMaker SDK. Pipelines should be created using SageMaker Projects in production. When creating a Project, two AWS CodeCommit repositories are created - model-build and model-deploy. These are integrated with AWS EventBridge and CodePipeline to allow automatic execution of pipelines upon code commits. This helps to achieve GitOps principles and manage CI/CD of projects.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The model-build pipeline (aka the CI part)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This section discusses the use of MLflow to track model training runs in pipeline.py, a two-step build pipeline for Iris datasets. The first step is PrepareIrisData and the second step is IrisTuning, which uses SageMaker HyperparameterTuner to train models and select one with the highest accuracy score. When the pipeline starts running, MLflow UI reports data for the runs and conditionally registers models whose accuracy score is above 0.9. Conditions can be further extended to include multiple evaluation scores or create multiple tiers of model quality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The model-deploy pipeline (aka the CD part)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The MLflow Model Registry is used to store qualified models, after which the Data Science team can inspect and evaluate them to decide which model version should be deployed. This decision is encoded in the buildspec.yml of the model-deploy repository and when a commit push is made, an automated CodePipeline workflow will deploy the model to a SageMaker Endpoint. Finally, the endpoint can be tested with sample feature data to get class predictions, thereby completing the CI/CD process with MLflow integration.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Conclusion"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "MLflow is an MLOps tool that offers advantages over traditional methods for data project collaboration. It provides a centralized workspace, model registry, deployment integrations and a REST API to manage the entire machine learning lifecycle. It is suitable for starting new projects or improving existing ones.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of article: 2427 words\n",
      "Length of final summary: 1025 words\n"
     ]
    }
   ],
   "source": [
    "# Examples of articles\n",
    "\n",
    "url = \"https://medium.com/sfu-cspmp/mlflow-a-modern-mlops-tool-for-data-project-collaboration-704ca299d9c3\"\n",
    "display_summary(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# SqueezeNet: The Key to Unlocking the Potential of Edge Computing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Written by **Data Hunters** on **Feb 9**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## AI’s New Frontier: The Rise of SqueezeNet"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### What is SqueezeNet?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a compact and efficient convolutional neural network algorithm designed to strike a balance between high accuracy and low complexity. It was proposed in 2016 by researchers from DeepScale, UC Berkeley, and Stanford University and consists of Fire modules which combine 1x1 and 3x3 filters to reduce the number of parameters while maintaining high accuracy. SqueezeNet has had numerous advancements and improvements, including SqueezeNet v1.1 and v2.0, which have increased accuracy while decreasing computational expenses. This blog post explores the intricacies of SqueezeNet architecture, its performance and technical nuances, as well as potential applications and possibilities within artificial intelligence.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Unlocking the Secret: A Look into SqueezeNet’s Inner Workings"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "AlexNet is a deep learning model that revolutionized computer vision when it was introduced in 2012. SqueezeNet offers an advantage by being able to run on edge devices with limited computational resources due to its 50 times fewer parameters and 10 times fewer FLOPS than AlexNet. This is achieved through channel squeezing, fire modules and deep compression, which all help reduce the network's computational cost without compromising accuracy.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### SqueezeNet and its Predecessors"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a CNN that offers an optimal balance between accuracy and computational resources, making it suitable for use on embedded systems or mobile devices. It has been recognized for its architectural innovations, which have been adopted by other CNN architectures to boost performance. It has also served as a starting point for designing other efficient architectures, highlighting its potential to inspire future new and innovative architectures.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Which is better: A Comparison Between SqueezeNet and AlexNet?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "AlexNet and SqueezeNet were both implemented and trained on a dataset from Kaggle consisting of 200 images of different soil types. AlexNet is a deep neural network with 8 layers, including 5 convolutional layers and 3 fully connected layers. After training, the final training loss, training accuracy, validation loss, and validation accuracy were observed.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### SqueezeNet"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a lightweight alternative to AlexNet, which it is based on. For our implementation, we utilized the first version for an image classification task. It contains a fire module with compressed layers and a standalone convolution layer (conv1) layered with 8 fire modules and a final convolution layer. Max pooling of stride 2 is applied for Conv1, fire4, fire8 and conv10, and a dropout of ratio 50% is applied in the end. When trained with the soil dataset, SqueezeNet outperformed AlexNet in performance and required just 9.07 MB of space, compared to 374.25 MB for AlexNet.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Deploying SqueezeNet: Unleashing its Power on Various Platforms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a deep-learning architecture that can be used on both memory and computationally constrained devices and cloud-based platforms. Its small model size can be further reduced through quantization and pruning. Quantization decreases the precision of a neural network's weights to reduce the model's memory footprint and speed up computations. Pruning removes unnecessary connections and neurons, shrinking the model size and reducing unnecessary information. These methods are well described in research papers by Jacob et al and Han et al.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evolution: A Look at SqueezeNet Over the Years"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet, first introduced in 2016, has gone through several changes and improvements to its network structure, such as the addition of new layers and increasing the number of filters in certain layers. This resulted in improved accuracy with significantly reduced computation required by a factor of 2.4x compared to the original SqueezeNet 1.0, while having slightly fewer parameters. The ongoing development and evolution of the architecture shows its potential for further performance gains.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### SqueezeNeXt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNeXt is a convolutional neural network based on the SqueezeNet architecture that aims to improve performance and reduce model size. To do this it uses a two-stage squeeze module for more aggressive channel reduction, replaces 3x3 convolutions with separable 3x3 convolutions, removes the 1x1 branch after the squeeze module, and incorporates an element-wise addition skip connection from ResNet architecture.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Squeeze Net in Action: Real-world examples of SqueezeNet"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a popular, efficient architecture used for various applications in computer vision, such as image classification, object detection and semantic segmentation. Additionally, its versatility has enabled its use in other fields, including healthcare and self-driving cars. In the latter, SqueezeNet has been used to improve the accuracy and efficiency of real-time object detection.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Face-Mask Detection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is an attractive choice for face mask detection due to its ability to reduce model size while maintaining or surpassing high accuracy. In a study, SqueezeNet was compared with the YOLO V2 network and results showed that it allowed for a reduction in the number of parameters and model size, making it more suitable for storage and transfer. The authors recommend replacing Darknet with SqueezeNet for improved performance in face mask detection, which can help to increase the accuracy and efficiency of this task and can be useful in security, surveillance, and healthcare.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Outdoor Power Monitoring"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "C. Fang and team proposed an improved version of SqueezeNet for outdoor power monitoring which includes an increase in the input size of the first convolution layer, a reduction in the convolution kernel size, global average pooling, small fully connected layers, and batch normalization. Evaluated using a multi-weather image dataset, the algorithm outperformed the original SqueezeNet in terms of accuracy and suppressed overfitting.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is being used in the medical imaging field to improve real-time image processing and diagnosis, leading to improved patient care. This technology is utilized in CT and MRI scanners for quicker picture analysis and diagnosis.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## The Ups and Downs: Navigating Challenges and Limitations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet has the advantage of being efficient and fast, but this comes at a cost of reduced accuracy compared to larger, more complex models. Additionally, its limited number of layers and filters can limit its ability to learn complex features and representations. Furthermore, its scalability is limited and it may not be compatible with other architectures, resulting in limited usefulness in certain applications.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Wrapping it Up: The Final Verdict"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "SqueezeNet is a valuable tool for improving efficiency and accuracy in a wide range of real-world applications, from self-driving cars to medical imaging. It is well-suited for these applications due to its ability to perform real-time object detection and classification while consuming minimal computational resources.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of article: 2057 words\n",
      "Length of final summary: 940 words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://medium.com/sfu-cspmp/squeezenet-the-key-to-unlocking-the-potential-of-edge-computing-c8b224d839ba\"\n",
    "display_summary(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6d51dcbe9226e36e47c812d5d2da8a1c45ac44a36cd09df90827668bad70539"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
