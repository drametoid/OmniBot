{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Only for Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9eeWUhAWRFcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_JOzZG-LMbNi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as nnF\n",
        "import torchtext.functional as F\n",
        "import torch.nn as nn\n",
        "import torchtext.transforms as T\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "\n",
        "class LSTM_Predictor:\n",
        "  def __init__(self, model_path):\n",
        "    self.model_path = model_path\n",
        "    self.padding_idx = 1\n",
        "    self.bos_idx = 0\n",
        "    self.eos_idx = 2\n",
        "    self.max_seq_len = 256\n",
        "    self.xlmr_vocab_path = r\"https://download.pytorch.org/models/text/xlmr.vocab.pt\"\n",
        "    self.xlmr_spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n",
        "\n",
        "    self.text_transform = T.Sequential(\n",
        "        T.SentencePieceTokenizer(self.xlmr_spm_model_path),\n",
        "        T.VocabTransform(load_state_dict_from_url(self.xlmr_vocab_path)),\n",
        "        T.Truncate(self.max_seq_len - 2),\n",
        "        T.AddToken(token=self.bos_idx, begin=True),\n",
        "        T.AddToken(token=self.eos_idx, begin=False),\n",
        "    )\n",
        "    self.vocab = self.text_transform[1].vocab.vocab\n",
        "    self.word_to_idx = self.vocab.get_stoi()\n",
        "\n",
        "  def maybe_gpu(self, v, gpu):\n",
        "      return v.cuda() if gpu else v\n",
        "\n",
        "  def get_pretrained_embeddings(self, hp):\n",
        "    glove_vectors = GloVe(name=\"6B\", dim=hp['EMBEDDING_DIM'])\n",
        "    EMBEDDING_DIM = glove_vectors.vectors.shape[1]\n",
        "    pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(self.vocab), EMBEDDING_DIM)).astype('f')\n",
        "    pretrained_embeddings[0] = 0\n",
        "    for word, wi in glove_vectors.stoi.items():\n",
        "        try:\n",
        "            pretrained_embeddings[self.word_to_idx[word]-1] = glove_vectors.__getitem__(word)\n",
        "        except KeyError:\n",
        "            pass\n",
        "    pretrained_embeddings = self.maybe_gpu(torch.from_numpy(pretrained_embeddings), hp['USE_GPU'])\n",
        "    return pretrained_embeddings\n",
        "\n",
        "  def get_lstm_model(parent_self):\n",
        "    class LSTMSwitcher(nn.Module):\n",
        "\n",
        "        def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size,\n",
        "                    use_gpu, batch_size, dropout=0.5, bidirectional=False, classifier_head=None):\n",
        "            \"\"\"Prepare individual layers\"\"\"\n",
        "            super(LSTMSwitcher, self).__init__()\n",
        "            self.hidden_dim = hidden_dim\n",
        "            self.use_gpu = use_gpu\n",
        "            self.batch_size = batch_size\n",
        "            self.dropout = dropout\n",
        "            self.num_directions = 2 if bidirectional else 1\n",
        "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=bidirectional)\n",
        "            self.hidden2label = nn.Linear(hidden_dim*self.num_directions, label_size)\n",
        "            self.hidden = self.init_hidden()\n",
        "            self.classifier_head = classifier_head\n",
        "\n",
        "        def init_hidden(self, batch_size=None):\n",
        "            \"\"\"Choose appropriate size and type of hidden layer\"\"\"\n",
        "            if not batch_size:\n",
        "                batch_size = self.batch_size\n",
        "            what = torch.zeros\n",
        "            return (parent_self.maybe_gpu(Variable(what(self.num_directions, batch_size, self.hidden_dim)), self.use_gpu),\n",
        "                    parent_self.maybe_gpu(Variable(what(self.num_directions, batch_size, self.hidden_dim)), self.use_gpu))\n",
        "\n",
        "        def classify(self, features):\n",
        "            y = self.hidden2label(features)\n",
        "            log_probs = nnF.log_softmax(y, dim=1)\n",
        "            return log_probs\n",
        "\n",
        "        def forward(self, sentence):\n",
        "            \"\"\"Use the layers of this model to propagate input and return class log probabilities\"\"\"\n",
        "            if self.use_gpu:\n",
        "                sentence = sentence.cuda()\n",
        "            x = self.embeddings(sentence).permute(1,0,2)\n",
        "            batch_size = x.shape[1]\n",
        "            self.hidden = self.init_hidden(batch_size=batch_size)\n",
        "            lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "            features = lstm_out[-1]\n",
        "            return self.classify(features)\n",
        "    return LSTMSwitcher\n",
        "\n",
        "  def prepare_model(self, LSTMSwitcher, hp, pretrained_embeddings):\n",
        "    num_classes = 5\n",
        "    switcher = LSTMSwitcher(embedding_dim=hp['EMBEDDING_DIM'], hidden_dim=hp['HIDDEN_DIM'],\n",
        "                                vocab_size=len(self.vocab), label_size=num_classes,\\\n",
        "                                use_gpu=hp['USE_GPU'], batch_size=hp['BATCH_SIZE'], dropout=hp['DROPOUT'], bidirectional=hp['USE_BILSTM'])\n",
        "    switcher.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "    model = switcher\n",
        "    return model\n",
        "\n",
        "  def load_model(self):\n",
        "    hp = {\n",
        "        \"EPOCHS\": 10,\n",
        "        \"DROPOUT\": .01,\n",
        "        \"HIDDEN_DIM\": 50,\n",
        "        \"BATCH_SIZE\": 16,\n",
        "        \"USE_BILSTM\": True,\n",
        "        \"LEARNING_RATE\": 1e-2,\n",
        "        \"EMBEDDING_DIM\": 50,\n",
        "        \"USE_GPU\": torch.cuda.is_available()\n",
        "    }\n",
        "    pretrained_embeddings = self.get_pretrained_embeddings(hp)\n",
        "    LSTMSwitcher = self.get_lstm_model()\n",
        "    self.model = self.prepare_model(LSTMSwitcher, hp, pretrained_embeddings)\n",
        "    self.model.load_state_dict(torch.load(self.model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "  def get_prediction(self, text):\n",
        "    with torch.no_grad():\n",
        "      classes = ['audio_to_text_data', 'blog_scrapping', 'random_data', 'resume_summary_data', 'suduko_data']\n",
        "      self.model.eval()\n",
        "      text = self.text_transform([text])\n",
        "      text_tensor = F.to_tensor(text, self.padding_idx)\n",
        "      prediction = self.model(text_tensor)\n",
        "      probs = torch.softmax(prediction, dim=-1)[0].tolist()\n",
        "      final_prediction = list(zip(classes, probs))\n",
        "      final_prediction = sorted(final_prediction, key = lambda x: -x[1])\n",
        "      return final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Predictor only once at server start up\n",
        "MODEL_PATH = 'drive/MyDrive/LSTM.pt'\n",
        "predictor = LSTM_Predictor(MODEL_PATH)\n",
        "predictor.load_model()"
      ],
      "metadata": {
        "id": "ngW98RSrSS75"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run prediction for each API call\n",
        "predictor.get_prediction('Solve this Sudoku puzzle for me')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q0_WJPxP9J7",
        "outputId": "8d811855-7dce-4973-8630-f71634a95c1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('suduko_data', 0.9787524342536926),\n",
              " ('resume_summary_data', 0.00814287830144167),\n",
              " ('audio_to_text_data', 0.006867147982120514),\n",
              " ('blog_scrapping', 0.005731482058763504),\n",
              " ('random_data', 0.0005060606054030359)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}